{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                date             user  \\\n",
       "0             0  1467810369 2009-04-06 22:19:45  _TheSpecialOne_   \n",
       "1             0  1467810672 2009-04-06 22:19:49    scotthamilton   \n",
       "2             0  1467810917 2009-04-06 22:19:53         mattycus   \n",
       "3             0  1467811184 2009-04-06 22:19:57          ElleCTF   \n",
       "4             0  1467811193 2009-04-06 22:19:57           Karoli   \n",
       "...         ...         ...                 ...              ...   \n",
       "1599995       4  2193601966 2009-06-16 08:40:49  AmandaMarie1028   \n",
       "1599996       4  2193601969 2009-06-16 08:40:49      TheWDBoards   \n",
       "1599997       4  2193601991 2009-06-16 08:40:49           bpbabe   \n",
       "1599998       4  2193602064 2009-06-16 08:40:49     tinydiamondz   \n",
       "1599999       4  2193602129 2009-06-16 08:40:50   RyanTrevMorris   \n",
       "\n",
       "                                                      text  \n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1        is upset that he can't update his Facebook by ...  \n",
       "2        @Kenichan I dived many times for the ball. Man...  \n",
       "3          my whole body feels itchy and like its on fire   \n",
       "4        @nationwideclass no, it's not behaving at all....  \n",
       "...                                                    ...  \n",
       "1599995  Just woke up. Having no school is the best fee...  \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the column names since the file doesn't have a header\n",
    "column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# Read input data\n",
    "tweets = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin-1', names=column_names, header=None)\n",
    "\n",
    "# Drop 'flag' as it won't be of use\n",
    "tweets = tweets.drop(['flag'], axis=1)\n",
    "\n",
    "# Remove the timezone abbreviation (PDT) from the 'date' column as all are PDT\n",
    "tweets['date'] = tweets['date'].str.replace('PDT', '')\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "tweets['date'] = pd.to_datetime(tweets['date'], format='%a %b %d %H:%M:%S %Y')\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into smaller parts\n",
    "\n",
    "tweet = tweets.text.values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "# Association between words and assigned number\n",
    "\n",
    "tokenizer.fit_on_texts(tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the words with their assigned numbers\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "# Pad sentences to have equal length\n",
    "\n",
    "padded_sequence = pad_sequences(encoded_docs, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 32)           22110752  \n",
      "                                                                 \n",
      " spatial_dropout1d (Spatial  (None, 200, 32)           0         \n",
      " Dropout1D)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50)                16600     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22127403 (84.41 MB)\n",
      "Trainable params: 22127403 (84.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using Long Short Term Memory Networks to work with words in large texts\n",
    "# This uses dropout to drop some neurons and avoid overfitting\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
