{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541200</th>\n",
       "      <td>0</td>\n",
       "      <td>2200003196</td>\n",
       "      <td>2009-06-16 18:18:12</td>\n",
       "      <td>LaLaLindsey0609</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0</td>\n",
       "      <td>1467998485</td>\n",
       "      <td>2009-04-06 23:11:14</td>\n",
       "      <td>sexygrneyes</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766711</th>\n",
       "      <td>0</td>\n",
       "      <td>2300048954</td>\n",
       "      <td>2009-06-23 13:40:11</td>\n",
       "      <td>sammydearr</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285055</th>\n",
       "      <td>0</td>\n",
       "      <td>1993474027</td>\n",
       "      <td>2009-06-01 10:26:07</td>\n",
       "      <td>Lamb_Leanne</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705995</th>\n",
       "      <td>0</td>\n",
       "      <td>2256550904</td>\n",
       "      <td>2009-06-20 12:56:51</td>\n",
       "      <td>yogicerdito</td>\n",
       "      <td>upper airways problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953018</th>\n",
       "      <td>4</td>\n",
       "      <td>1824563004</td>\n",
       "      <td>2009-05-17 01:55:54</td>\n",
       "      <td>snedwan</td>\n",
       "      <td>@Yorksville no I'm seeing them at Hampden in G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993770</th>\n",
       "      <td>4</td>\n",
       "      <td>1835439669</td>\n",
       "      <td>2009-05-18 05:59:58</td>\n",
       "      <td>beeveekay</td>\n",
       "      <td>Brr- it's 61 in San antonio in May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937940</th>\n",
       "      <td>4</td>\n",
       "      <td>1793332552</td>\n",
       "      <td>2009-05-14 02:16:55</td>\n",
       "      <td>joshh01</td>\n",
       "      <td>@craiggower i know this sounds random: i'm ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208691</th>\n",
       "      <td>0</td>\n",
       "      <td>1973785775</td>\n",
       "      <td>2009-05-30 11:49:53</td>\n",
       "      <td>A1nz</td>\n",
       "      <td>@Stephaniekaren wants to leave me for this dude.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977400</th>\n",
       "      <td>4</td>\n",
       "      <td>1833735285</td>\n",
       "      <td>2009-05-18 00:16:25</td>\n",
       "      <td>jemappellejemma</td>\n",
       "      <td>@ewwerik SLEEPING PILLLLZZ! ;O Just kidding, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                date             user  \\\n",
       "541200       0  2200003196 2009-06-16 18:18:12  LaLaLindsey0609   \n",
       "750          0  1467998485 2009-04-06 23:11:14      sexygrneyes   \n",
       "766711       0  2300048954 2009-06-23 13:40:11       sammydearr   \n",
       "285055       0  1993474027 2009-06-01 10:26:07      Lamb_Leanne   \n",
       "705995       0  2256550904 2009-06-20 12:56:51      yogicerdito   \n",
       "...        ...         ...                 ...              ...   \n",
       "953018       4  1824563004 2009-05-17 01:55:54          snedwan   \n",
       "993770       4  1835439669 2009-05-18 05:59:58        beeveekay   \n",
       "937940       4  1793332552 2009-05-14 02:16:55          joshh01   \n",
       "208691       0  1973785775 2009-05-30 11:49:53             A1nz   \n",
       "977400       4  1833735285 2009-05-18 00:16:25  jemappellejemma   \n",
       "\n",
       "                                                     text  \n",
       "541200             @chrishasboobs AHHH I HOPE YOUR OK!!!   \n",
       "750     @misstoriblack cool , i have no tweet apps  fo...  \n",
       "766711  @TiannaChaos i know  just family drama. its la...  \n",
       "285055  School email won't open  and I have geography ...  \n",
       "705995                             upper airways problem   \n",
       "...                                                   ...  \n",
       "953018  @Yorksville no I'm seeing them at Hampden in G...  \n",
       "993770                Brr- it's 61 in San antonio in May   \n",
       "937940  @craiggower i know this sounds random: i'm ver...  \n",
       "208691  @Stephaniekaren wants to leave me for this dude.   \n",
       "977400  @ewwerik SLEEPING PILLLLZZ! ;O Just kidding, d...  \n",
       "\n",
       "[800000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the column names since the file doesn't have a header\n",
    "column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# Read input data\n",
    "tweets = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin-1', names=column_names, header=None)\n",
    "\n",
    "# Drop 'flag' as it won't be of use\n",
    "tweets = tweets.drop(['flag'], axis=1)\n",
    "\n",
    "# Remove the timezone abbreviation (PDT) from the 'date' column as all are PDT\n",
    "tweets['date'] = tweets['date'].str.replace('PDT', '')\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "tweets['date'] = pd.to_datetime(tweets['date'], format='%a %b %d %H:%M:%S %Y')\n",
    "\n",
    "# Define the desired subset size (e.g., 20% of the original data)\n",
    "subset_size = int(0.5 * len(tweets))  # Adjust as needed\n",
    "\n",
    "# Randomly sample the subset\n",
    "tweets = tweets.sample(n=subset_size, random_state=42)\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into smaller parts\n",
    "\n",
    "tweet = tweets.text.values\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "# Association between words and assigned number\n",
    "\n",
    "tokenizer.fit_on_texts(tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the words with their assigned numbers\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "# Pad sentences to have equal length\n",
    "\n",
    "padded_sequence = pad_sequences(encoded_docs, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 32)           13826528  \n",
      "                                                                 \n",
      " spatial_dropout1d (Spatial  (None, 200, 32)           0         \n",
      " Dropout1D)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50)                16600     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13843179 (52.81 MB)\n",
      "Trainable params: 13843179 (52.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using Long Short Term Memory Networks to work with words in large texts\n",
    "# This uses dropout to drop some neurons and avoid overfitting\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2500/2500 [==============================] - 594s 237ms/step - loss: -68.9969 - accuracy: 1.3750e-04 - val_loss: -127.6614 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "2500/2500 [==============================] - 592s 237ms/step - loss: -185.9409 - accuracy: 0.0000e+00 - val_loss: -243.3843 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "2500/2500 [==============================] - 622s 249ms/step - loss: -317.4928 - accuracy: 0.0200 - val_loss: -399.5653 - val_accuracy: 0.0600\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "history = model.fit(padded_sequence, tweets['target'], validation_split=.9, epochs=3, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
